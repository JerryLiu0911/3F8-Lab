%% LyX 2.2.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{textcomp}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage{float}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

\makeatother

\usepackage{babel}
\begin{document}

\title{3F8: Inference\\
 Short Lab Report}

\author{Author's Name}
\maketitle
\begin{abstract}
This is the abstract. 

Try for 1-2 sentences on each of: motive (what it's
about), method (what was

done), key results and conclusions (the main outcomes).

\textbullet{} Don't exceed 3 sentences on any one.

\textbullet{} Write this last, when you know what it should say!
\end{abstract}

\section{Introduction}
\begin{enumerate}
\item What is the problem and why is it interesting?
\item What novel follow-up will the rest of your report present?
\end{enumerate}

\section{Exercise a)\label{sec:Exercise-a}}

In this exercise we have to consider the logistic classication model
(aka logistic regression) and derive the gradients of the log-likelihood
given a vector of binary labels $\mathbf{y}$ and a matrix of input
features $\mathbf{X}$. The gradient of the log-likelihood can be
writen as
\[
\frac{\partial\mathcal{L}(\beta)}{\partial\beta}=\sum_{i=1}^{n}(y_i - \sigma(\mathbf{x}_i^\top\beta))\mathbf{\tilde{x}}_i\,.
\]
where $\sigma(\cdot)$ is the sigmoid function and $\mathbf{\tilde{x}}_i$ is the $i$-th row of the feature matrix $\mathbf{X}$ with an additional bias term. This allows us to update the parameters $\beta$ using gradient ascent as :
\[
\beta \leftarrow \beta + \eta \sum_{i=1}^{n}(y_i - \sigma(\mathbf{x}_i^\top\beta))\mathbf{\tilde{x}}_i\,.
\]
or in vectorised form as:
\[
\beta \leftarrow \beta + \eta \mathbf{X}^\top(\mathbf{y} - \sigma(\mathbf{X}\beta))\,.
\]
where $\eta$ is the learning rate parameter.

\section{Exercise b)}

In this exercise we are asked to write pseudocode to estimate the
parameters $\beta$ using gradient ascent of the log-likelihood. Our
code should be vectorised. The pseudocode to estimate the parameters
$\beta$ is shown below:
\begin{verbatim}
Function estimate_parameters:

   Input:  feature matrix X, labels y
   Output: vector of coefficients b
	
   Code:
   for t in 1 to n_steps do:
      b = b + learning_rate * X^T * (y - sigmoid(X*b))
   return b
\end{verbatim}
The learning rate parameter $\eta$ is chosen to be 0.3 as to reduce overshooting, and the number of steps of gradient ascent is set to be 1000. 

\section{Exercise c)}

In this exercise we visualise the dataset in the two-dimensional input
space displaying each datapoint's class label. The dataset is visualised
in Figure \ref{fig:data_visualisation}. By analising Figure \ref{fig:data_visualisation}
we conclude that a linear classifier may struggle to separate the two classes, as there is a significant overlap between the two classes in the input space. 
However, we can also see that there are some regions in the input space where one class is more dominant than the other, which suggests that a linear classifier may still be able to find a decision boundary that separates the two classes to some extent.
Overall, while a linear classifier may not be able to perfectly separate the two classes, it may still be able to achieve reasonable performance on this dataset.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.3\paperwidth]{data_visualisation.png}
\par\end{centering}
\caption{Visualisation of the data.\label{fig:data_visualisation} }
\end{figure}


\section{Exercise d)}

In this exercise we split the data randomly into training and test
sets with 800 and 200 data points, respectively. The pseudocode from
exercise a) is transformed into python code as follows:
\begin{verbatim}
def fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha):
    w = np.random.randn(X_tilde_train.shape[ 1 ]) # initialise w randomly
    ll_train = np.zeros(n_steps) 
    ll_test = np.zeros(n_steps)
    for i in range(n_steps):
        sigmoid_value = predict(X_tilde_train, w)
        log_likelihood_gradient = np.dot(X_tilde_train.T, (y_train - sigmoid_value)) / X_tilde_train.shape[ 0 ] 
        w = w + alpha * log_likelihood_gradient
        ll_train[ i ] = compute_average_ll(X_tilde_train, y_train, w)
        ll_test[ i ] = compute_average_ll(X_tilde_test, y_test, w)
        print(ll_train[ i ], ll_test[ i ])

    return w, ll_train, ll_test
\end{verbatim}

We then train the classifier using this code. We fixed the learning
rate parameter to be $\eta=0.3$ as to prevent over-shooting. The average log-likelihood on the
training and test sets as the optimisation proceeds are shown in Figure
\ref{fig:learning_curves}. By looking at these plots we conclude
that the likelihood plateaus very early, likely a consequence of the limitations of a linear boundary. 

Figure \ref{fig:data_visualisation-1} displays the visualisation of the
contours of the class predictive probabilities on top of the data. We can see that the limitations of the
linear classifier is evident, as we find that the boundary is clearly unable to separate the clusters of the two classes
with one single boundary, resulting in a boundary with high uncertainty as seen by the spread of the contours.


\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.3\paperwidth]{learning_curves_train.png}\hspace{1cm}\includegraphics[width=0.3\paperwidth]{learning_curves_test.png}
\par\end{centering}
\caption{Learning curves showing the average log-likelihood on the training
(left) and test (right) datasets.\label{fig:learning_curves} }
\end{figure}

\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.3\paperwidth]{linear_contour_visualisation.png}
\par\end{centering}
\caption{Visualisation of the contours of the class predictive probabilities.\label{fig:data_visualisation-1} }
\end{figure}


\section{Exercise e)}

The final average training and test log-likelihoods are shown in Table
\ref{tab:average_ll}. The 2x2 confusion
matrices on the test set is shown in Table \ref{tab:confusion_test}.
By analising this table, we conclude that the model is able to correctly classify 74\% of the positive class and 71\% of the negative class, which indicates that the model has a decent performance on the test set. We also found that through our experiments, 
there are often fluctuations of whether the performance favours class 1 or class 2, which may be due to the permutation function chosen at the beginning of the code to split the data into training and test sets. 
This suggests that the performance of the model may be sensitive to the specific data points included in the training and test sets, which highlights the importance of using a robust method for splitting the data and evaluating the model's performance.
For the results obtained here, we found that the training set of 800 samples contained 391 samples of class 2 and 409 samples of class 1, while the test set of 200 samples contained 109 samples of class 1 and 91 samples of class 0.
This balanced distribution of classes in both the training and test sets may have contributed to the balanced performance on both classes, as the model had sufficient examples of each class to learn from during training and to be evaluated on during testing.
\begin{table}[H]
\centering{}%
\begin{minipage}[t]{0.49\columnwidth}%
\begin{center}
\begin{tabular}{c|c}
\textbf{Avg. Train ll} & \textbf{Avg. Test ll}\tabularnewline
\hline 
-0.6271 & -0.6093 \tabularnewline
\hline 
\end{tabular} 
\par\end{center}
\caption{Average training and test log-likelihoods to 4 s.f.\label{tab:average_ll}}
%
\end{minipage}%
\begin{minipage}[t]{0.49\columnwidth}%
\begin{center}
\begin{tabular}{cc|c|c}
 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\hat{y}$} & \tabularnewline
 &  & 0 & 1\tabularnewline
\cline{2-4} 
$y$ & 0 & 0.71 & 0.29\tabularnewline
\cline{2-4} 
 & 1 & 0.26 & 0.74 \tabularnewline
\cline{2-4} 
\end{tabular} 
\par\end{center}
\caption{Confusion matrix on the test set.\label{tab:confusion_test}}
%
\end{minipage}
\end{table}


\section{Exercise f)}

We now expand the inputs through a set of Gaussian radial basis functions
centred on the training datapoints. We consider widths $l=\{0.01,0.1,1\}$
for the basis functions. We fix the learning rate parameter again to be
$\eta=\{0.3,0.3,0.03\}$ for each $l=\{0.01,0.1,1\}$, with number of training steps $n=\{3000,8000,30000\}$, respectively.
The learning rate and number of training steps are chosen ad-hoc such that the log-likelihood plateaus for each choice of $l$, while reducing over-shooting for larger values of $l$.
Figure \ref{fig:contours_l} displays the visualisation of the contours
of the resulting class predictive probabilities on top of the data
for each choice of $l=\{0.01,0.1,1\}$. The choice of the width $l$ has a significant impact on the resulting decision boundary. 
When $l$ is too small (e.g. $l=0.01$), the decision boundary becomes very complex and overfits the training data, resulting in a highly non-linear boundary that may not generalise well to new data points.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.32\textwidth]{contours_l_001.png}\hspace{0.15cm}\includegraphics[width=0.32\textwidth]{contours_l_01.png}\hspace{0.15cm}\includegraphics[width=0.32\textwidth]{contours_l_1.png}
\par\end{centering}
\caption{Visualisation of the contours of the class predictive probabilities
for $l=0.01$ (left), $l=0.1$ (middle), $l=1$ (right).\label{fig:contours_l} }
\end{figure}


\section{Exercise g)}

The final final training and test log-likelihoods per datapoint obtained
for each setting of $l=\{0.01,0.1,1\}$ are shown in tables \ref{tab:avg_ll_l_001},
\ref{tab:avg_ll_l_01} and \ref{tab:avg_ll_l_1}.  The 2 \texttimes{} 2 confusion matrices for the three models
trained with $l=\{0.01,0.1,1\}$ are show in tables \ref{tab:conf_l_001},
\ref{tab:conf_l_01} and \ref{tab:conf_l_1}. After analysing these
matrices, we can say that... When we compare these results to those
obtained using the original inputs we conclude that...

\begin{table}[H]
\centering{}%
\begin{minipage}[t]{0.3\textwidth}%
\begin{center}
\begin{tabular}{c|c}
\textbf{Avg. Train ll} & \textbf{Avg. Test ll}\tabularnewline
\hline 
-0.4814 & -0.6802\tabularnewline
\hline 
\end{tabular}\caption{Results for $l=0.01$\label{tab:avg_ll_l_001}}
\par\end{center}%
\end{minipage}\hspace{0.5cm}%
\begin{minipage}[t]{0.3\textwidth}%
\begin{center}
\begin{tabular}{c|c}
\textbf{Avg. Train ll} & \textbf{Avg. Test ll}\tabularnewline
\hline 
-0.1419 & -0.2223\tabularnewline
\hline 
\end{tabular}\caption{Results for $l=0.1$\label{tab:avg_ll_l_01}}
\par\end{center}%
\end{minipage}\hspace{0.5cm}%
\begin{minipage}[t]{0.3\textwidth}%
\begin{center}
\begin{tabular}{c|c}
\textbf{Avg. Train ll} & \textbf{Avg. Test ll}\tabularnewline
\hline 
-0.2073 & -0.2007\tabularnewline
\hline 
\end{tabular}\caption{Results for $l=1$\label{tab:avg_ll_l_1}}
\par\end{center}%
\end{minipage}
\end{table}

\begin{table}
\centering{}%
\begin{minipage}[t]{0.33\textwidth}%
\begin{center}
\begin{tabular}{cc|c|c}
 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\hat{y}$} & \tabularnewline
 &  & 0 & 1\tabularnewline
\cline{2-4} 
$y$ & 0 & 0.97 & 0.03\tabularnewline
\cline{2-4} 
 & 1 & 0.93 & 0.07\tabularnewline
\cline{2-4} 
\end{tabular} 
\par\end{center}
\caption{Conf. matrix $l=0.01$.\label{tab:conf_l_001}}
%
\end{minipage}%
\begin{minipage}[t]{0.33\textwidth}%
\begin{center}
\begin{tabular}{cc|c|c}
 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\hat{y}$} & \tabularnewline
 &  & 0 & 1\tabularnewline
\cline{2-4} 
$y$ & 0 & 0.96 & 0.04\tabularnewline
\cline{2-4} 
 & 1 & 0.08 & 0.92\tabularnewline
\cline{2-4} 
\end{tabular} 
\par\end{center}
\caption{Conf. matrix $l=0.1$.\label{tab:conf_l_01}}
%
\end{minipage}%
\begin{minipage}[t]{0.33\textwidth}%
\begin{center}
\begin{tabular}{cc|c|c}
 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\hat{y}$} & \tabularnewline
 &  & 0 & 1\tabularnewline
\cline{2-4} 
$y$ & 0 & 0.94 & 0.06\tabularnewline
\cline{2-4} 
 & 1 & 0.06 & 0.94\tabularnewline
\cline{2-4} 
\end{tabular} 
\par\end{center}
\caption{Conf. matrix $l=1$.\label{tab:conf_l_1}}
%
\end{minipage}
\end{table}


\section{Conclusions}
\begin{enumerate}
\item Draw together the most important results and their consequences.
\item List any reservations or limitations.
\end{enumerate}

\end{document}
